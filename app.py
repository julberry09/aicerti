import os
import json
import logging
from typing import TypedDict, List, Dict, Any, Optional
from pathlib import Path

from dotenv import load_dotenv
from fastapi import FastAPI, Body, Request
from pydantic import BaseModel
import uvicorn
import httpx
import time as _time

from langchain.schema import Document
from langchain_community.document_loaders import PyPDFLoader, CSVLoader, TextLoader, Docx2txtLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, END
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from starlette.middleware.base import BaseHTTPMiddleware

# =============================================================
# 1. ê³µí†µ ì„¤ì • / í™˜ê²½ ë³€ìˆ˜
# =============================================================
load_dotenv()

logger = logging.getLogger("helpdesk-bot")


# êµ¬ì¡°í™” ë¡œê·¸(JSON)
LOG_DIR = Path("./logs"); LOG_DIR.mkdir(parents=True, exist_ok=True)
class _ConsoleFormatter(logging.Formatter):
    def format(self, record):
        base = {"level": record.levelname, "name": record.name, "msg": record.getMessage()}
        if hasattr(record, "extra_data"):
            base.update(record.extra_data)
        return json.dumps(base, ensure_ascii=False)

console_handler = logging.StreamHandler()
console_handler.setFormatter(_ConsoleFormatter())
file_handler = logging.FileHandler(LOG_DIR / "app.log", encoding="utf-8")
file_handler.setFormatter(_ConsoleFormatter())
logger.handlers = []
logger.setLevel(logging.INFO)
logger.addHandler(console_handler)
logger.addHandler(file_handler)

# Azure OpenAI í™˜ê²½ë³€ìˆ˜
AOAI_ENDPOINT    = os.getenv("AOAI_ENDPOINT", "")
AOAI_API_KEY     = os.getenv("AOAI_API_KEY", "")
AOAI_API_VERSION = os.getenv("AOAI_API_VERSION", "2024-10-21")
AOAI_DEPLOY_GPT4O_MINI = os.getenv("AOAI_DEPLOY_GPT4O_MINI", "gpt-4o-mini")
AOAI_DEPLOY_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O", "gpt-4o")
AOAI_DEPLOY_EMBED_3_SMALL = os.getenv("AOAI_DEPLOY_EMBED_3_SMALL", "text-embedding-3-small")

# Webhook ì•Œë¦¼
NOTIFY_WEBHOOK_URL = os.getenv("NOTIFY_WEBHOOK_URL", "")
NOTIFY_ENABLED = bool(NOTIFY_WEBHOOK_URL)

# ê²½ë¡œ
KB_DIR = Path("./kb")
INDEX_DIR = Path("./index")
INDEX_NAME = "faiss_index"

# ìƒ˜í”Œ ë°ì´í„°
OWNER_FALLBACK = {
    "ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìê´€ë¦¬": {"owner": "í™ê¸¸ë™", "email": "owner.hr@example.com", "phone": "010-1234-5678"},
    "ì¬ë¬´ì‹œìŠ¤í…œ-ì •ì‚°í™”ë©´": {"owner": "ê¹€ì¬ë¬´", "email": "owner.fa@example.com", "phone": "010-2222-3333"},
    "í¬í„¸-ê³µì§€ì‘ì„±": {"owner": "ë°•ìš´ì˜", "email": "owner.ops@example.com", "phone": "010-9999-0000"},
}
EMPLOYEE_DIR = {
    "kim.s": {"name": "ê¹€ì„ ë‹ˆ", "dept": "ITìš´ì˜", "phone": "010-1111-2222", "status": "active"},
    "lee.a": {"name": "ì´ì•ŒíŒŒ", "dept": "ë³´ì•ˆ", "phone": "010-3333-4444", "status": "active"},
}

# =============================================================
# 2. ìœ í‹¸
# =============================================================
def _is_running_in_streamlit() -> bool:
    try:
        import streamlit as st
        from streamlit.runtime.scriptrunner import get_script_run_ctx
        return get_script_run_ctx() is not None
    except Exception:
        return False

def notify(event: str, data: dict | None = None):
    payload = {"event": event, "data": data or {}}
    try:
        logger.info("notify", extra={"extra_data": {"event": event, "data": data or {}}})
        if NOTIFY_ENABLED:
            import requests
            requests.post(NOTIFY_WEBHOOK_URL, json=payload, timeout=3)
    except Exception as e:
        logger.warning(f"ë…¸í‹° ì‹¤íŒ¨: {e}")

# =============================================================
# 3. RAG ìœ í‹¸ë¦¬í‹°
# =============================================================
def _make_embedder() -> AzureOpenAIEmbeddings:
    return AzureOpenAIEmbeddings(
        azure_deployment=AOAI_DEPLOY_EMBED_3_SMALL,
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
        api_version=AOAI_API_VERSION,
    )

def _load_docs_from_kb() -> List[Document]:
    docs: List[Document] = []
    if not KB_DIR.exists():
        KB_DIR.mkdir(parents=True, exist_ok=True)
    for p in KB_DIR.rglob("*"):
        if p.is_file():
            try:
                suf = p.suffix.lower()
                if suf == ".pdf":
                    docs.extend(PyPDFLoader(str(p)).load())
                elif suf == ".csv":
                    docs.extend(CSVLoader(file_path=str(p), encoding="utf-8").load())
                elif suf in [".txt", ".md"]:
                    docs.extend(TextLoader(str(p), encoding="utf-8").load())
                elif suf == ".docx":
                    docs.extend(Docx2txtLoader(str(p)).load())
            except Exception as e:
                logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {p} - {e}")
    return docs

def build_or_load_vectorstore() -> FAISS:
    embed = _make_embedder()
    if (INDEX_DIR / f"{INDEX_NAME}.faiss").exists():
        return FAISS.load_local(str(INDEX_DIR / INDEX_NAME), embeddings=embed, allow_dangerous_deserialization=True)

    raw_docs = _load_docs_from_kb()
    if not raw_docs:
        seed_text = """ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ì•ˆë‚´
- ID ë°œê¸‰: ì‹ ê·œ ì…ì‚¬ìëŠ” HR í¬í„¸ì—ì„œ 'ê³„ì • ì‹ ì²­' ì–‘ì‹ì„ ì œì¶œ. ìŠ¹ì¸ í›„ ITê°€ ê³„ì • ìƒì„±.
- ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”: SSO í¬í„¸ì˜ 'ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •' ê¸°ëŠ¥ ì‚¬ìš©. ë³¸ì¸ì¸ì¦ í•„ìš”.
- ë‹´ë‹¹ì ì¡°íšŒ: í¬í„¸ ìƒë‹¨ ê²€ìƒ‰ì°½ì— í™”ë©´/ë©”ë‰´ëª…ì„ ì…ë ¥í•˜ë©´ ë‹´ë‹¹ì ì¹´ë“œê°€ í‘œì‹œë¨.
- ê·¼ë¬´ì‹œê°„: í‰ì¼ 09:00~18:00, ì ì‹¬ 12:00~13:00
- ê¸´ê¸‰ ì—°ë½: it-help@example.com / 02-123-4567
"""
        raw_docs = [Document(page_content=seed_text, metadata={"source": "seed-faq.txt"})]

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)
    chunks = splitter.split_documents(raw_docs)
    INDEX_DIR.mkdir(parents=True, exist_ok=True)
    vs = FAISS.from_documents(chunks, embed)
    vs.save_local(str(INDEX_DIR / INDEX_NAME))
    return vs

_vectorstore: Optional[FAISS] = None
def retriever(k: int = 4):
    global _vectorstore
    if _vectorstore is None:
        _vectorstore = build_or_load_vectorstore()
    return _vectorstore.as_retriever(search_kwargs={"k": k})

def make_llm(model: str = AOAI_DEPLOY_GPT4O_MINI, temperature: float = 0.2) -> AzureChatOpenAI:
    if not (AOAI_ENDPOINT and AOAI_API_KEY):
        raise RuntimeError("AOAI_ENDPOINT/AOAI_API_KEY ë¯¸ì„¤ì •. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”.")
    return AzureChatOpenAI(
        azure_deployment=model,
        api_version=AOAI_API_VERSION,
        azure_endpoint=AOAI_ENDPOINT,
        api_key=AOAI_API_KEY,
        temperature=temperature,
    )

# =============================================================
# 4. LangGraph (ë„êµ¬ + ë…¸ë“œ)
# =============================================================
class BotState(TypedDict):
    question: str
    intent: str
    result: str
    sources: List[Dict[str, Any]]
    tool_output: Dict[str, Any]

# ---- ë„êµ¬ ----
def tool_reset_password(payload: Dict[str, Any]) -> Dict[str, Any]:
    user = payload.get("user") or ""
    found = EMPLOYEE_DIR.get(user)
    if not found:
        return {
            "ok": False,
            "message": "ì‚¬ë²ˆ/ê³„ì •ì´ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í¬í„¸ì˜ 'ë¹„ë°€ë²ˆí˜¸ ì°¾ê¸°'ì—ì„œ ì‚¬ë²ˆ/ì‚¬ë‚´ë©”ì¼ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "steps": ["SSO í¬í„¸ ì ‘ì† > ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •", "ë³¸ì¸ì¸ì¦(íœ´ëŒ€í°/ì´ë©”ì¼)", "ìƒˆ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •"],
            "contact": {"email": "it-help@example.com", "phone": "02-123-4567"},
        }
    return {
        "ok": True,
        "message": f"{found['name']}ë‹˜ì˜ ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì ˆì°¨",
        "steps": ["SSO í¬í„¸ ì ‘ì† > ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •", "ë³¸ì¸ì¸ì¦", "ìƒˆ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •"],
        "contact": {"email": "it-help@example.com", "phone": "02-123-4567"},
    }

def tool_request_id(payload: Dict[str, Any]) -> Dict[str, Any]:
    ticket_no = f"REQ-{int(_time.time())}"
    return {
        "ok": True,
        "message": "ID ë°œê¸‰ ì‹ ì²­ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "ticket": ticket_no,
        "next": ["HR í¬í„¸ ì‹ ì²­", "ë¶€ì„œì¥ ìŠ¹ì¸", "IT ê³„ì • ìƒì„±"],
        "sla": "1~2ì¼",
    }

def tool_owner_lookup(payload: Dict[str, Any]) -> Dict[str, Any]:
    screen = payload.get("screen") or ""
    info = OWNER_FALLBACK.get(screen)
    if not info:
        return {"ok": False, "message": f"'{screen}' ë‹´ë‹¹ì ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}
    return {"ok": True, "screen": screen, "owner": info}

# ---- ë…¸ë“œ ----
def node_classify(state: BotState) -> BotState:
    llm = make_llm()
    sys_prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ë¼ìš°í„°ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ì ì…ë ¥ì„ reset_password, request_id, owner_lookup, rag_qa ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”. "
        "JSON(intent, arguments)ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”."
    )
    msg = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": state["question"]}]
    out = llm.invoke(msg).content
    intent, args = "rag_qa", {}
    try:
        data = json.loads(out)
        intent = data.get("intent", "rag_qa")
        args = data.get("arguments", {}) or {}
    except Exception:
        pass
    return {**state, "intent": intent, "tool_output": args}

def node_reset_pw(state: BotState) -> BotState:
    args = state.get("tool_output", {}) or {}
    res = tool_reset_password(args)
    return {**state, "tool_output": res}

def node_request_id(state: BotState) -> BotState:
    args = state.get("tool_output", {}) or {}
    res = tool_request_id(args)
    return {**state, "tool_output": res}

def node_owner_lookup(state: BotState) -> BotState:
    args = state.get("tool_output", {}) or {}
    res = tool_owner_lookup(args)
    return {**state, "tool_output": res}

def node_rag(state: BotState) -> BotState:
    r = retriever(k=4)
    docs = r.get_relevant_documents(state["question"])
    context = "\n\n".join([f"[{i+1}] {d.page_content[:1200]}" for i, d in enumerate(docs)])
    sources = [{"index": i+1, "source": d.metadata.get("source","unknown"), "page": d.metadata.get("page")} for i,d in enumerate(docs)]
    llm = make_llm(model=AOAI_DEPLOY_GPT4O)
    sys_prompt = "ë„ˆëŠ” ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ìƒë‹´ì›ì´ë‹¤. ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ë¼."
    user_prompt = f"ì§ˆë¬¸:\n{state['question']}\n\nì»¨í…ìŠ¤íŠ¸:\n{context}"
    out = llm.invoke([{"role":"system","content":sys_prompt},{"role":"user","content":user_prompt}]).content
    return {**state, "result": out, "sources": sources}

def node_finalize(state: BotState) -> BotState:
    if state["intent"] in ["reset_password", "request_id", "owner_lookup"]:
        res = state.get("tool_output", {})
        if state["intent"] == "reset_password":
            if res.get("ok"):
                text = "âœ… ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i,s in enumerate(res.get("steps", [])))
            else:
                text = "â—" + res.get("message","ì‹¤íŒ¨")
        elif state["intent"] == "request_id":
            text = f"ğŸ†” ID ë°œê¸‰ ì‹ ì²­\nìƒíƒœ: {'ì ‘ìˆ˜ë¨' if res.get('ok') else 'ì‹¤íŒ¨'}\ní‹°ì¼“: {res.get('ticket','-')}"
        else:
            if res.get("ok"):
                o = res.get("owner", {})
                text = f"ğŸ‘¤ '{res.get('screen')}' ë‹´ë‹¹ì\n- ì´ë¦„: {o.get('owner')}\n- ì´ë©”ì¼: {o.get('email')}\n- ì—°ë½ì²˜: {o.get('phone')}"
            else:
                text = "â—" + res.get("message","ì¡°íšŒ ì‹¤íŒ¨")
        return {**state, "result": text}
    return state

def build_graph():
    g = StateGraph(BotState)
    g.add_node("classify", node_classify)
    g.add_node("reset_password", node_reset_pw)
    g.add_node("request_id", node_request_id)
    g.add_node("owner_lookup", node_owner_lookup)
    g.add_node("rag", node_rag)
    g.add_node("finalize", node_finalize)
    g.set_entry_point("classify")

    def _route(state: BotState):
        m = state["intent"]
        if m == "reset_password": return "reset_password"
        if m == "request_id": return "request_id"
        if m == "owner_lookup": return "owner_lookup"
        return "rag"

    g.add_conditional_edges("classify", _route, {
        "reset_password":"finalize","request_id":"finalize","owner_lookup":"finalize","rag":"rag"})
    g.add_edge("finalize", END); g.add_edge("rag", END)
    return g.compile()

_graph = None
def pipeline(question: str) -> Dict[str, Any]:
    global _graph
    logger.info("pipeline_in", extra={"extra_data": {"q": question}})
    if _graph is None:
        _graph = build_graph()
    state: BotState = {"question": question, "intent":"", "result":"", "sources":[], "tool_output":{}}
    out = _graph.invoke(state)
    logger.info("pipeline_out", extra={"extra_data": {"intent": out.get("intent","")}})
    return out

# =============================================================
# 5. FastAPI
# =============================================================
api = FastAPI(title="Helpdesk RAG API", version="0.1.0")

class AuditMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start = _time.time()
        logger.info("api_request", extra={"extra_data": {"path": request.url.path}})
        try:
            response = await call_next(request)
            dur = round((_time.time() - start)*1000)
            logger.info("api_response", extra={"extra_data": {"status": response.status_code, "ms": dur}})
            return response
        except Exception as e:
            logger.exception("api_error", extra={"extra_data": {"error": str(e)}})
            raise

api.add_middleware(AuditMiddleware)

class ChatIn(BaseModel): message: str
class ChatOut(BaseModel): reply: str; intent: str; sources: List[Dict[str, Any]]= []

@api.get("/health")
def health(): return {"ok":True}

@api.post("/chat", response_model=ChatOut)
def chat(payload: ChatIn = Body(...)):
    out = pipeline(payload.message)
    return ChatOut(reply=out.get("result",""), intent=out.get("intent",""), sources=out.get("sources", []))

# =============================================================
# 6. Streamlit UI
# =============================================================
def run_streamlit_ui():
    import streamlit as st

    st.set_page_config(page_title="ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ RAG", page_icon="ğŸ’¬", layout="wide")
    st.title("ğŸ’¼ ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ì±—ë´‡ (RAG + LangGraph)")

    with st.sidebar:
        st.header("ğŸ“š ì§€ì‹ë² ì´ìŠ¤(KB)")
        if st.button("ì¸ë±ìŠ¤ ì¬ë¹Œë“œ"):
            try:
                # Clear previous index
                for ext in [".faiss", ".pkl"]:
                    p = INDEX_DIR / f"{INDEX_NAME}{ext}"
                    if p.exists():
                        p.unlink()
                st.info("ì¸ë±ìŠ¤ ì¬ìƒì„± ì¤‘...")
                build_or_load_vectorstore()
                st.success("ì™„ë£Œ!")
            except Exception as e:
                st.error(f"ì‹¤íŒ¨: {e}")
        uploaded = st.file_uploader("ë¬¸ì„œ ì—…ë¡œë“œ (PDF/CSV/TXT/DOCX)", type=["pdf","csv","txt","md","docx"], accept_multiple_files=True)
        if uploaded:
            KB_DIR.mkdir(parents=True, exist_ok=True)
            for f in uploaded:
                dest = KB_DIR / f.name
                with open(dest, "wb") as w:
                    w.write(f.read())
            st.success(f"{len(uploaded)}ê°œ ë¬¸ì„œ ì €ì¥ë¨. 'ì¸ë±ìŠ¤ ì¬ë¹Œë“œ'ë¥¼ ëˆŒëŸ¬ ë°˜ì˜í•˜ì„¸ìš”.")

        st.divider()
        api_host = os.getenv("API_CLIENT_HOST", "localhost")
        api_port = int(os.getenv("API_PORT", 8000))
        api_base_url = f"http://{api_host}:{api_port}"

        # st.header("âš™ ì„¤ì •")
        # ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í† ê¸€ UIì˜ í…ìŠ¤íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        use_api = st.toggle(f"ë°±ì—”ë“œ API ì‚¬ìš© ({api_base_url}/chat)", value=False)
        #use_api = st.toggle("ë°±ì—”ë“œ API ì‚¬ìš© (http://localhost:8000/chat)", value=False)
        st.caption("ë¹„í™œì„±í™” ì‹œ ë¡œì»¬ íŒŒì´í”„ë¼ì¸ ì§ì ‘ í˜¸ì¶œ")

    if "chat" not in st.session_state:
        st.session_state.chat = []

    for role, content in st.session_state.chat:
        with st.chat_message(role):
            st.markdown(content)

    if q := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ì˜ˆ: 'ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”', 'ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìê´€ë¦¬ ë‹´ë‹¹ì'"):



        st.session_state.chat.append(("user", q))
        with st.chat_message("user"):
            st.markdown(q)

        with st.chat_message("assistant"):
            with st.spinner("ì²˜ë¦¬ ì¤‘..."):
                try:
                    if use_api:
                        with httpx.Client(timeout=30.0) as client:
                            resp = client.post(api_base_url, json={"message": q})
                            resp.raise_for_status()
                            data = resp.json()
                            reply = data.get("reply","")
                            sources = data.get("sources", [])
                    else:
                        out = pipeline(q)
                        reply = out.get("result","")
                        sources = out.get("sources", [])
                    st.markdown(reply)
                    if sources:
                        with st.expander("ğŸ” ì°¸ì¡° ì†ŒìŠ¤"):
                            for s in sources:
                                line = f"- [{s.get('index')}] {s.get('source')}"
                                if s.get("page") is not None:
                                    line += f" (page {s['page']})"
                                st.write(line)
                    st.session_state.chat.append(("assistant", reply))
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜: {e}")

# Auto-run Streamlit UI only when truly inside Streamlit runner

# =============================================================
# 7. ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# =============================================================
if _is_running_in_streamlit():
    run_streamlit_ui()

# CLI entry for FastAPI
if __name__ == "__main__":
    import argparse
    default_host = os.getenv("API_SERVER_HOST", "0.0.0.0")
    default_port = int(os.getenv("API_PORT", 8000))

    parser = argparse.ArgumentParser()
    parser.add_argument("--api", action="store_true", help="Run FastAPI server")
    parser.add_argument("--host", default=default_host)
    parser.add_argument("--port", default=default_port, type=int)
    args = parser.parse_args()

    if args.api:
        uvicorn.run(api, host=args.host, port=args.port)
    else:
        print("Usage:")
        print("  FastAPI : python app.py --api")
        print("  UI      : streamlit run app.py")
