"""
app.py â€” Streamlit UI + LangGraph pipeline + optional FastAPI backend for an internal Helpdesk RAG chatbot

Features
- Streamlit-based chat UI (run: `streamlit run app.py`)
- FastAPI backend serving /chat (run: `python app.py --api`)
- RAG: loads docs from ./kb into FAISS; supports PDF, CSV, TXT, DOCX
- LangChain + LangGraph: intent routing (reset_password, request_id, owner_lookup, rag_qa)
- Azure OpenAI (AOAI) via langchain-openai (Chat + Embeddings)

Env (examples)
AOAI_ENDPOINT=https://123
AOAI_API_KEY=123
AOAI_API_VERSION=2024-10-21
AOAI_DEPLOY_GPT4O_MINI=gpt-4o-mini
AOAI_DEPLOY_GPT4O=gpt-4o
AOAI_DEPLOY_EMBED_3_LARGE=text-embedding-3-large
AOAI_DEPLOY_EMBED_3_SMALL=text-embedding-3-small
AOAI_DEPLOY_EMBED_ADA=text-embedding-ada-002
"""

import os
import sys
import json
import time
import shutil
import logging
from typing import TypedDict, List, Dict, Any, Optional

from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# --- Streamlit detection must be lazy to avoid issues when importing in FastAPI/test contexts ---
def _is_running_in_streamlit() -> bool:
    try:
        import streamlit as st  # noqa
        # Streamlit >=1.32
        from streamlit.runtime.scriptrunner import get_script_run_ctx
        return get_script_run_ctx() is not None
    except Exception:
        return False

# --- Logging ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("helpdesk-bot")

# --- LangChain / LangGraph / Vector store ---
from pathlib import Path
from langchain.schema import Document
from langchain_community.document_loaders import PyPDFLoader, CSVLoader, TextLoader, Docx2txtLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import StateGraph, END

# OpenAI (Azure) via LangChain wrappers
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings

# Backend
from fastapi import FastAPI, Body
from pydantic import BaseModel
import uvicorn

# Optional client for UI to call API
import httpx

# ---- ENV ----
AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT", "")
AOAI_API_KEY = os.getenv("AOAI_API_KEY", "")
AOAI_API_VERSION = os.getenv("AOAI_API_VERSION", "2024-10-21")

DEP_GPT4O_MINI = os.getenv("AOAI_DEPLOY_GPT4O_MINI", "gpt-4o-mini")
DEP_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O", "gpt-4o")
DEP_EMB_LARGE = os.getenv("AOAI_DEPLOY_EMBED_3_LARGE", "text-embedding-3-large")
DEP_EMB_SMALL = os.getenv("AOAI_DEPLOY_EMBED_3_SMALL", "text-embedding-3-small")

# ---- Paths ----
KB_DIR = Path("./kb")
INDEX_DIR = Path("./index")
INDEX_NAME = "faiss_index"

# ---- Sample directory data (owners & users) for demo tools ----
OWNER_FALLBACK = {
    "ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìê´€ë¦¬": {"owner": "í™ê¸¸ë™", "email": "owner.hr@example.com", "phone": "010-1234-5678"},
    "ì¬ë¬´ì‹œìŠ¤í…œ-ì •ì‚°í™”ë©´": {"owner": "ê¹€ì¬ë¬´", "email": "owner.fa@example.com", "phone": "010-2222-3333"},
    "í¬í„¸-ê³µì§€ì‘ì„±": {"owner": "ë°•ìš´ì˜", "email": "owner.ops@example.com", "phone": "010-9999-0000"},
}
EMPLOYEE_DIR = {
    "kim.s": {"name": "ê¹€ì„ ë‹ˆ", "dept": "ITìš´ì˜", "phone": "010-1111-2222", "status": "active"},
    "lee.a": {"name": "ì´ì•ŒíŒŒ", "dept": "ë³´ì•ˆ", "phone": "010-3333-4444", "status": "active"},
}

# ========================
# Vector store (RAG) utils
# ========================
def _make_embedder():
    if not (AOAI_ENDPOINT and AOAI_API_KEY):
        raise RuntimeError("AOAI_ENDPOINT/AOAI_API_KEY ë¯¸ì„¤ì •. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”.")
    return AzureOpenAIEmbeddings(
        azure_deployment=DEP_EMB_SMALL,
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
        api_version=AOAI_API_VERSION,
    )

def _load_docs_from_kb() -> List[Document]:
    docs: List[Document] = []
    if not KB_DIR.exists():
        KB_DIR.mkdir(parents=True, exist_ok=True)
    for p in KB_DIR.rglob("*"):
        if p.is_dir():
            continue
        try:
            if p.suffix.lower() == ".pdf":
                docs.extend(PyPDFLoader(str(p)).load())
            elif p.suffix.lower() == ".csv":
                docs.extend(CSVLoader(file_path=str(p), encoding="utf-8").load())
            elif p.suffix.lower() in [".txt", ".md"]:
                docs.extend(TextLoader(str(p), encoding="utf-8").load())
            elif p.suffix.lower() in [".docx"]:
                docs.extend(Docx2txtLoader(str(p)).load())
        except Exception as e:
            logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {p} - {e}")
    return docs

def build_or_load_vectorstore() -> FAISS:
    embed = _make_embedder()
    index_path = INDEX_DIR / INDEX_NAME
    if (INDEX_DIR / f"{INDEX_NAME}.faiss").exists() and (INDEX_DIR / f"{INDEX_NAME}.pkl").exists():
        logger.info("ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ")
        return FAISS.load_local(str(INDEX_DIR / INDEX_NAME), embeddings=embed, allow_dangerous_deserialization=True)
    # Build
    raw_docs = _load_docs_from_kb()
    if not raw_docs:
        # If kb empty, seed with a tiny internal FAQ so the bot can still answer
        seed_text = """ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ì•ˆë‚´
- ID ë°œê¸‰: ì‹ ê·œ ì…ì‚¬ìëŠ” HR í¬í„¸ì—ì„œ 'ê³„ì • ì‹ ì²­' ì–‘ì‹ì„ ì œì¶œ. ìŠ¹ì¸ í›„ ITê°€ ê³„ì • ìƒì„±.
- ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”: SSO í¬í„¸ì˜ 'ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •' ê¸°ëŠ¥ ì‚¬ìš©. ë³¸ì¸ì¸ì¦ í•„ìš”.
- ë‹´ë‹¹ì ì¡°íšŒ: í¬í„¸ ìƒë‹¨ ê²€ìƒ‰ì°½ì— í™”ë©´/ë©”ë‰´ëª…ì„ ì…ë ¥í•˜ë©´ ë‹´ë‹¹ì ì¹´ë“œê°€ í‘œì‹œë¨.
- ê·¼ë¬´ì‹œê°„: í‰ì¼ 09:00~18:00, ì ì‹¬ 12:00~13:00
- ê¸´ê¸‰ ì—°ë½: it-help@example.com / 02-123-4567
"""
        raw_docs = [Document(page_content=seed_text, metadata={"source": "seed-faq.txt"})]
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)
    chunks = splitter.split_documents(raw_docs)
    vs = FAISS.from_documents(chunks, embed)
    vs.save_local(str(INDEX_DIR / INDEX_NAME))
    return vs

# Global singletons (lazy)
_vectorstore: Optional[FAISS] = None
def retriever(k: int = 4):
    global _vectorstore
    if _vectorstore is None:
        _vectorstore = build_or_load_vectorstore()
    return _vectorstore.as_retriever(search_kwargs={"k": k})

def make_llm(model: str = DEP_GPT4O_MINI, temperature: float = 0.2):
    if not (AOAI_ENDPOINT and AOAI_API_KEY):
        raise RuntimeError("AOAI_ENDPOINT/AOAI_API_KEY ë¯¸ì„¤ì •. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”.")
    return AzureChatOpenAI(
        azure_deployment=model,
        api_version=AOAI_API_VERSION,
        azure_endpoint=AOAI_ENDPOINT,
        api_key=AOAI_API_KEY,
        temperature=temperature,
    )

# ===============
# LangGraph State
# ===============
class BotState(TypedDict):
    question: str
    intent: str
    result: str
    sources: List[Dict[str, Any]]
    # raw tool outputs (optional)
    tool_output: Dict[str, Any]

# ============
# Tool functions
# ============
def tool_reset_password(payload: Dict[str, Any]) -> Dict[str, Any]:
    user = payload.get("user") or ""
    # In reality you'd send an email/sms OOB link or call an IAM API.
    found = EMPLOYEE_DIR.get(user)
    if not found:
        return {
            "ok": False,
            "message": "ì‚¬ë²ˆ/ê³„ì •ì´ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í¬í„¸ì˜ 'ë¹„ë°€ë²ˆí˜¸ ì°¾ê¸°'ì—ì„œ ì‚¬ë²ˆ/ì‚¬ë‚´ë©”ì¼ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "steps": [
                "SSO í¬í„¸ ì ‘ì† > ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •",
                "ë³¸ì¸ì¸ì¦(íœ´ëŒ€í°/ì´ë©”ì¼)",
                "ìƒˆ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •"
            ]
        }
    return {
        "ok": True,
        "message": f"{found['name']}ë‹˜ì˜ ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì ˆì°¨ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.",
        "steps": [
            "SSO í¬í„¸ ì ‘ì† > ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •",
            "ë³¸ì¸ì¸ì¦(íœ´ëŒ€í°/ì´ë©”ì¼)",
            "ìƒˆ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •",
            "ë§Œì•½ ì‹¤íŒ¨ ì‹œ it-help@example.com ìœ¼ë¡œ ë¬¸ì˜"
        ]
    }

def tool_request_id(payload: Dict[str, Any]) -> Dict[str, Any]:
    name = payload.get("name") or "ì‹ ê·œ ì…ì‚¬ì"
    dept = payload.get("dept") or "ë¯¸ì •"
    # Simulate ticket creation
    ticket_no = f"REQ-{int(time.time())}"
    return {
        "ok": True,
        "message": f"{name}({dept}) ID ë°œê¸‰ ì‹ ì²­ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "ticket": ticket_no,
        "next": [
            "HR í¬í„¸ 'ê³„ì • ì‹ ì²­' ì–‘ì‹ ì œì¶œ",
            "ë¶€ì„œì¥ ìŠ¹ì¸",
            "IT ê³„ì • ìƒì„± ë° ì´ˆê¸° ì•ˆë‚´ ë©”ì¼ ë°œì†¡"
        ]
    }

def tool_owner_lookup(payload: Dict[str, Any]) -> Dict[str, Any]:
    screen = payload.get("screen") or ""
    info = OWNER_FALLBACK.get(screen)
    if not info:
        # fallback: fuzzy search
        keys = [k for k in OWNER_FALLBACK.keys() if screen and screen in k]
        if keys:
            info = OWNER_FALLBACK[keys[0]]
    if not info:
        return {"ok": False, "message": f"'{screen}' ë‹´ë‹¹ì ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í¬í„¸ ê²€ìƒ‰ ë˜ëŠ” í—¬í”„ë°ìŠ¤í¬ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”."}
    return {"ok": True, "screen": screen, "owner": info}

# ===================
# LangGraph definition
# ===================
def node_classify(state: BotState) -> BotState:
    llm = make_llm()
    sys_prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ë¼ìš°í„°ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ì ì…ë ¥ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ê³  JSONìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. "
        "labels: reset_password, request_id, owner_lookup, rag_qa\n"
        "JSON keys: intent, arguments\n"
        "argumentsëŠ” dictë¡œ ì¶”ì¶œí•œ ìŠ¬ë¡¯(ì˜ˆ: user, name, dept, screen ë“±)ì„ ë‹´ìŠµë‹ˆë‹¤."
    )
    msg = [{"role": "system", "content": sys_prompt},
           {"role": "user", "content": state["question"]}]
    out = llm.invoke(msg).content
    intent, args = "rag_qa", {}
    try:
        data = json.loads(out)
        intent = data.get("intent", "rag_qa")
        args = data.get("arguments", {}) or {}
    except Exception:
        pass
    return {**state, "intent": intent, "tool_output": args}

def node_reset_pw(state: BotState) -> BotState:
    args = state.get("tool_output", {}) or {}
    res = tool_reset_password(args)
    return {**state, "tool_output": res}

def node_request_id(state: BotState) -> BotState:
    args = state.get("tool_output", {}) or {}
    res = tool_request_id(args)
    return {**state, "tool_output": res}

def node_owner_lookup(state: BotState) -> BotState:
    args = state.get("tool_output", {}) or {}
    res = tool_owner_lookup(args)
    return {**state, "tool_output": res}

def node_rag(state: BotState) -> BotState:
    # retrieve
    r = retriever(k=4)
    docs = r.get_relevant_documents(state["question"])
    # compose
    context = "\n\n".join([f"[{i+1}] {d.page_content[:1200]}" for i, d in enumerate(docs)])
    sources = []
    for i, d in enumerate(docs):
        src = d.metadata.get("source", "unknown")
        page = d.metadata.get("page", None)
        sources.append({"index": i+1, "source": src, "page": page})
    llm = make_llm(model=DEP_GPT4O)
    sys_prompt = (
        "ë„ˆëŠ” ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ìƒë‹´ì›ì´ë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•´ ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ë¼. "
        "ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ì¼ë°˜ íšŒì‚¬ ê·œì •ì˜ ìƒì‹ ë²”ìœ„ ë‚´ì—ì„œ ì‹ ì¤‘íˆ ì¶”ë¡ í•˜ë˜, ê°€ì •ì€ ë¶„ëª…íˆ í‘œì‹œí•˜ë¼."
    )
    user_prompt = f"ì§ˆë¬¸:\n{state['question']}\n\nì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§€ì¹¨:\n- ë²ˆí˜¸ ë§¤ê²¨ ë‹¨ê³„ë¡œ ì ˆì°¨ë¥¼ ìš”ì•½\n- ê´€ë ¨ ë§í¬/ë‹´ë‹¹ë¶€ì„œ/ìš´ì˜ì‹œê°„ í¬í•¨(ìˆìœ¼ë©´)\n- 2~3ì¤„ì˜ í•µì‹¬ ìš”ì•½"
    out = llm.invoke([{"role": "system", "content": sys_prompt},
                      {"role": "user", "content": user_prompt}]).content
    return {**state, "result": out, "sources": sources}

def node_finalize(state: BotState) -> BotState:
    if state["intent"] in ["reset_password", "request_id", "owner_lookup"]:
        res = state.get("tool_output", {})
        # format nicely
        if state["intent"] == "reset_password":
            if res.get("ok"):
                text = "âœ… ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i, s in enumerate(res.get("steps", []))) + \
                    f"\n\nì°¸ê³ : {res.get('message','')}"
            else:
                text = "â—" + res.get("message", "ì²˜ë¦¬ ì‹¤íŒ¨")
        elif state["intent"] == "request_id":
            text = f"ğŸ†” ID ë°œê¸‰ ì‹ ì²­\n\nìƒíƒœ: {'ì ‘ìˆ˜ë¨' if res.get('ok') else 'ì‹¤íŒ¨'}\n" \
                   f"í‹°ì¼“: {res.get('ticket','-')}\n\në‹¤ìŒ ë‹¨ê³„:\n" + \
                   "\n".join(f"{i+1}. {s}" for i, s in enumerate(res.get('next', [])))
        else:
            if res.get("ok"):
                o = res.get("owner", {})
                text = f"ğŸ‘¤ '{res.get('screen')}' ë‹´ë‹¹ì\n- ì´ë¦„: {o.get('owner')}\n- ì´ë©”ì¼: {o.get('email')}\n- ì—°ë½ì²˜: {o.get('phone')}"
            else:
                text = "â—" + res.get("message", "ì¡°íšŒ ì‹¤íŒ¨")
        return {**state, "result": text}
    return state

# Build the graph
def build_graph():
    g = StateGraph(BotState)
    g.add_node("classify", node_classify)
    g.add_node("reset_password", node_reset_pw)
    g.add_node("request_id", node_request_id)
    g.add_node("owner_lookup", node_owner_lookup)
    g.add_node("rag", node_rag)
    g.add_node("finalize", node_finalize)

    g.set_entry_point("classify")

    def _route(state: BotState):
        m = state["intent"]
        if m == "reset_password":
            return "reset_password"
        if m == "request_id":
            return "request_id"
        if m == "owner_lookup":
            return "owner_lookup"
        return "rag"

    g.add_conditional_edges("classify", _route, {
        "reset_password": "finalize",
        "request_id": "finalize",
        "owner_lookup": "finalize",
        "rag": "rag",
    })
    g.add_edge("finalize", END)
    g.add_edge("rag", END)
    return g.compile()

_graph = None
def pipeline(question: str) -> Dict[str, Any]:
    global _graph
    if _graph is None:
        _graph = build_graph()
    state: BotState = {"question": question, "intent": "", "result": "", "sources": [], "tool_output": {}}
    out = _graph.invoke(state)
    return out

# ==============
# FastAPI server
# ==============
api = FastAPI(title="Helpdesk RAG API", version="0.1.0")

class ChatIn(BaseModel):
    message: str
    session_id: Optional[str] = "default"

class ChatOut(BaseModel):
    reply: str
    intent: str
    sources: List[Dict[str, Any]] = []

@api.get("/health")
def health():
    return {"ok": True}

@api.post("/chat", response_model=ChatOut)
def chat(payload: ChatIn = Body(...)):
    out = pipeline(payload.message)
    return ChatOut(reply=out.get("result",""), intent=out.get("intent",""), sources=out.get("sources", []))

# =================
# Streamlit UI app
# =================
def run_streamlit_ui():
    import streamlit as st

    st.set_page_config(page_title="ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ RAG", page_icon="ğŸ’¬", layout="wide")
    st.title("ğŸ’¼ ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ì±—ë´‡ (RAG + LangGraph)")

    with st.sidebar:
        st.header("ğŸ“š ì§€ì‹ë² ì´ìŠ¤(KB)")
        if st.button("ì¸ë±ìŠ¤ ì¬ë¹Œë“œ"):
            try:
                # Clear previous index
                for ext in [".faiss", ".pkl"]:
                    p = INDEX_DIR / f"{INDEX_NAME}{ext}"
                    if p.exists():
                        p.unlink()
                st.info("ì¸ë±ìŠ¤ ì¬ìƒì„± ì¤‘...")
                build_or_load_vectorstore()
                st.success("ì™„ë£Œ!")
            except Exception as e:
                st.error(f"ì‹¤íŒ¨: {e}")
        uploaded = st.file_uploader("ë¬¸ì„œ ì—…ë¡œë“œ (PDF/CSV/TXT/DOCX)", type=["pdf","csv","txt","md","docx"], accept_multiple_files=True)
        if uploaded:
            KB_DIR.mkdir(parents=True, exist_ok=True)
            for f in uploaded:
                dest = KB_DIR / f.name
                with open(dest, "wb") as w:
                    w.write(f.read())
            st.success(f"{len(uploaded)}ê°œ ë¬¸ì„œ ì €ì¥ë¨. 'ì¸ë±ìŠ¤ ì¬ë¹Œë“œ'ë¥¼ ëˆŒëŸ¬ ë°˜ì˜í•˜ì„¸ìš”.")

        st.divider()
        use_api = st.toggle("ë°±ì—”ë“œ API ì‚¬ìš© (http://localhost:8000/chat)", value=False)
        st.caption("ë¹„í™œì„±í™” ì‹œ ë¡œì»¬ íŒŒì´í”„ë¼ì¸ ì§ì ‘ í˜¸ì¶œ")

    if "chat" not in st.session_state:
        st.session_state.chat = []

    for role, content in st.session_state.chat:
        with st.chat_message(role):
            st.markdown(content)

    if q := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ì˜ˆ: 'ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”', 'ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìê´€ë¦¬ ë‹´ë‹¹ì'"):
        st.session_state.chat.append(("user", q))
        with st.chat_message("user"):
            st.markdown(q)

        with st.chat_message("assistant"):
            with st.spinner("ì²˜ë¦¬ ì¤‘..."):
                try:
                    if use_api:
                        with httpx.Client(timeout=30.0) as client:
                            resp = client.post("http://localhost:8000/chat", json={"message": q})
                            resp.raise_for_status()
                            data = resp.json()
                            reply = data.get("reply","")
                            sources = data.get("sources", [])
                    else:
                        out = pipeline(q)
                        reply = out.get("result","")
                        sources = out.get("sources", [])
                    st.markdown(reply)
                    if sources:
                        with st.expander("ğŸ” ì°¸ì¡° ì†ŒìŠ¤"):
                            for s in sources:
                                line = f"- [{s.get('index')}] {s.get('source')}"
                                if s.get("page") is not None:
                                    line += f" (page {s['page']})"
                                st.write(line)
                    st.session_state.chat.append(("assistant", reply))
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜: {e}")

# Auto-run Streamlit UI only when truly inside Streamlit runner
if _is_running_in_streamlit():
    run_streamlit_ui()

# CLI entry for FastAPI
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--api", action="store_true", help="Run FastAPI server")
    parser.add_argument("--host", default="0.0.0.0")
    parser.add_argument("--port", default=8000, type=int)
    args = parser.parse_args()

    if args.api:
        uvicorn.run(api, host=args.host, port=args.port)
    else:
        print("Usage:")
        print("  FastAPI : python app.py --api")
        print("  UI      : streamlit run app.py")
